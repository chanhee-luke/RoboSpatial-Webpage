---
layout: ../layouts/Layout.astro
title: "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics"
description: Webpage for RoboSpatial
favicon: favicon.ico
# thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

{/* Image imports */}
import fig1 from "../assets/fig1_new.png";
import fig2 from "../assets/fig2.png";
import fig3 from "../assets/fig3.png";
import robot_exp from "../assets/robot_exp.png";



<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Chan Hee Song",
      url: "https://chanh.ee",
      institution: "The Ohio State University",
      notes: ["*"],
    },
    {
      name: "Valts Blukis",
      institution: "Nvidia",
      url: "https://research.nvidia.com/person/valts-blukis",
      notes: [],
    },
    {
      name: "Jonathan Tremblay",
      institution: "Nvidia",
      url: "https://research.nvidia.com/person/jonathan-tremblay",
      notes: [],
    },
    {
      name: "Stephen Tyree",
      institution: "Nvidia",
      url: "https://research.nvidia.com/person/stephen-tyree",
    },
    {
      name: "Yu Su",
      url: "https://ysu1989.github.io/",
      institution: "The Ohio State University",
    },
    {
      name: "Stan Birchfield",
      url: "https://research.nvidia.com/person/stan-birchfield",
      institution: "Nvidia",
    },
  ]}
  conference=""
  notes={[
    {
      symbol: "*",
      text: "Work done during Nvidia internship",
    },
  ]}
  links={[
    {
      name: "ArXiv",
      url: "https://arxiv.org/abs/2411.16537",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "",
      icon: "mdi:github",
      disabled: true,
    },
    {
      name: "Dataset",
      url: "",
      icon: "mdi:database",
      disabled: true
    },
  ]}
  />

{/* <Video source={outside} /> */}

<HighlightedSection>

## Overview

**TL;DR** 1) A new training dataset and benchmark, **RoboSpatial** and **RoboSpatial-Home**, comprising images and 3D scans paired with spatial questions and answers, designed for robotics. 2) **RoboSpatial** trained model outperforms prior SOTA VLMs on natural language-specified robot manipulation tasks and indoor spatial scene question answering.

Spatial understanding is essential for robots to perceive, reason about, and interact with their environments. However, current visual language models often rely on general-purpose image datasets that lack robust spatial scene understanding and reference frame comprehension (ego-, object-, or world-centric). To address this gap, we introduce **RoboSpatial**, a large-scale dataset of **real indoor and tabletop environments** captured via egocentric images and 3D scans. RoboSpatial provides **1M images, 5K 3D scans, and 3M annotated spatial relationships**, enabling both 2D and 3D spatial reasoning. Models trained on RoboSpatial outperform baselines on tasks including spatial affordance prediction, spatial relationship prediction, and robotics manipulation.

</HighlightedSection>

## RoboSpatial Application Example

<Figure
    caption="An illustration of a model trained on RoboSpatial being used to solve a manipulation task using spatial reasoning."
  >
    <Image source={fig1} altText="An illustration of a model trained on RoboSpatial being used to solve a manipulation task using spatial reasoning." />
</Figure>

{/* ## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left" caption="Take a look at this YouTube video.">
    <YouTubeVideo videoId="wjZofJX0v4M" />
  </Figure>
  <Figure slot="right" caption="Now look at this Gaussian Splat, rendered with a React component.">
    <Splat client:idle />
  </Figure>
</TwoColumns> */}

## Overview

<Figure
    caption="We automatically generate spatial relationship annotations from existing datasets with 3D point clouds, egocentric images, and 3D bounding box annotations. We create question/answer pairs covering three classes of spatial relationships, three spatial reference frames, and both binary (yes/no) and numeric (e.g. 2D image points) answers. From 1M images and 1.4K scans, we generate over 3M spatial question/answer pairs."
  >
    <Image source={fig2} altText="An overview of RoboSpatial Dataset." />
</Figure>



## Evaluation Results

### Spatial QA

<Figure
    caption="Results of RoboSpatial-trained models on RoboSpatial validation set, RoboSpatial-Home and BLINK. Two models are shown: SL (SpaceLLaVA) and RP (RoboPoint), where the S- prefix and FT indicates RoboSpatial-trained models. For Yes/No questions, the green checkmark indicates the correct answer. For spatial context questions, GT indicates the correct answer. For in-domain, all questions are from RoboSpatial validation. For out-of-domain, all images except for the top right are from RoboSpatial-Home."
  >
    <Image source={fig3} altText="Qualitative Results of RoboSpatial-trained models." />
</Figure>

<Table caption="Performance of RoboSpatial-trained models on three spatial reasoning benchmarks.">
| Model                       | RoboSpatial Test | RoboSpatial-Home | BLINK-Spatial |
|:----------------------------|:---------------:|:---------------:|:---------------:|
| **Open-source**             |                |                 |                 |
| *--2D--*             |                |                 |                 |
| LLaVA-NeXT                  | 30.3           |23.7           |71.8           |
| + RoboSpatial               | 60.5    |  42.0               |    **79.0**             |
| RoboPoint                  | 38.9           |40.2           |  63.6           |
| + RoboSpatial               | 70.6    |  50.7               |    70.6             |
| *--3D--*             |                |                 |                 |
| Embodied Generalist        |     42.8     |       36.2          |     N/A            |
| + RoboSpatial               | **71.9**    |      **54.7**       |    N/A             |
| **Baselines**               |            |                 |                 |
| Molmo                     | 50.1           |   46.9        |   67.1          |
| GPT-4o                   | 50.8           |   48.6        |    76.2         |
</Table>

### Robot Manipulation

<Figure
    caption="Robotics experiments: the red dot shows the model output (if not present, the model failed to provide a valid point in the image);  green dots are used to show when a model outputs multiple points. The robot motion generator, cuRobo, is used to grasp the item referenced by the generated point. The spatial- prefix indicates model trained with RoboSpatial."
  >
    <Image source={robot_exp} altText="Robot experiment results." />
</Figure>

<Table caption="Task success rate for robot manipulation.">
| Model                        | Success Rate (%) |
|:----------------------------|:---------------:|
| **Open-source**              |                 |
| LLaVA-NeXT               | 23.7           |
| + RoboSpatial                | **52.6**    |
| **Baselines** |            |
| Molmo                     | 43.8           |
| GPT-4o                   | 46.9           |
</Table>

{/* | RoboPoint                 | 44.7           |
| + RoboSpatial                | 46.2        | */}


## BibTeX citation

```bibtex
@article{song2024robospatial,
  author  = {Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield},
  title   = {RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics},
  year    = {2024},
  journal = {arXiv preprint arXiv:2411.16537},
}
```